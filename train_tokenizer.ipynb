{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88090b63",
   "metadata": {},
   "source": [
    "## Make a Korean Tokenizer and merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19273d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d025976f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"../../datasets/privateLLM/test_data/\", \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc0bad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 158470\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86828651",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "for example in raw_datasets[\"test\"]:\n",
    "    for key in example.keys():\n",
    "        if key == 'input' and example.get(\"input\", \"\") == \"\":\n",
    "            continue\n",
    "        text_dict = {'text': example[key]}\n",
    "        sources.append(text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cfa6bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.dumps(sources, ensure_ascii=False)\n",
    "\n",
    "with open('../../datasets/privateLLM/test_data.jsonl', 'w', encoding='UTF-8') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7675063",
   "metadata": {},
   "source": [
    "## preprocess .json file to one sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93544796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"test\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"text\"]\n",
    "        \n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0cca5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_training_corpus at 0x0000027546379E00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c86e395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 158470\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "999298b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained('./tokenizertest/mistral_tokenizer', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53cb4c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " '마',\n",
       " '파',\n",
       " '두',\n",
       " '부',\n",
       " '의',\n",
       " '▁',\n",
       " '매',\n",
       " '력',\n",
       " '은',\n",
       " '▁',\n",
       " '어',\n",
       " '디',\n",
       " '까',\n",
       " '지',\n",
       " '인',\n",
       " '가',\n",
       " '?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"마파두부의 매력은 어디까지인가?\"\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b0ce362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b937b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22469fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d002079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0885c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1975e4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = list(vocab.keys())\n",
    "old_tokenizer.add_tokens(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28a97761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60378"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63a00d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁마', '파', '두', '부의▁', '매', '력은▁', '어디', '까지', '인가', '?']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afb8b278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁마', '파', '두', '부', '의', '▁매력', '은', '▁어디', '까', '지', '인', '가', '?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = old_tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99612861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./test_tokenizer3\\\\tokenizer_config.json',\n",
       " './test_tokenizer3\\\\special_tokens_map.json',\n",
       " './test_tokenizer3\\\\tokenizer.model',\n",
       " './test_tokenizer3\\\\added_tokens.json',\n",
       " './test_tokenizer3\\\\tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_tokenizer.save_pretrained(\"./test_tokenizer3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575c5a9",
   "metadata": {},
   "source": [
    "### Test Old & New Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9abb3caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1a', '2b', '3c', '4d', '5e']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources = ['1','2','3','4','5']\n",
    "targets = ['a','b','c','d','e']\n",
    "examples = [s + t for s, t in zip(sources, targets)]\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b2af66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_tokenized, sources_tokenized = [strings for strings in (examples, sources)]\n",
    "sources_tokenized\n",
    "# input_ids = examples_tokenized[\"input_ids\"]\n",
    "# labels = copy.deepcopy(input_ids)\n",
    "# for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "#     label[:source_len] = IGNORE_INDEX\n",
    "# return dict(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22ce6611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "operator.ne(sources, '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e923c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b56fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190e2515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "MODEL = 'beomi/KoAlpaca-Polyglot-5.8B'\n",
    "\n",
    "tokenizer_koalpaca = AutoTokenizer.from_pretrained(MODEL, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "886ad7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30003"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_koalpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9da56d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokenizer",
   "language": "python",
   "name": "tokenizer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
